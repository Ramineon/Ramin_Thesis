{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model 2",
   "id": "97b7528a4f4a4811"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T19:13:44.781006Z",
     "start_time": "2024-12-14T19:13:42.584026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import DataLoader, Subset, random_split, Dataset"
   ],
   "id": "2e67c708c52cbd9e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## initiation:",
   "id": "2784339f6be1f305"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T19:13:44.812866Z",
     "start_time": "2024-12-14T19:13:44.784016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NumpyFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []  # List to store file paths and their labels\n",
    "\n",
    "        # Traverse through the directory structure\n",
    "        for class_idx, class_name in enumerate(sorted(os.listdir(root_dir))):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for file_name in os.listdir(class_dir):\n",
    "                    if file_name.endswith('.npy'):\n",
    "                        file_path = os.path.join(class_dir, file_name)\n",
    "                        self.samples.append((file_path, class_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.samples[idx]\n",
    "        np_array = np.load(file_path).astype(np.float32)  # Load the .npy file\n",
    "\n",
    "        if self.transform:\n",
    "            np_array = self.transform(np_array)\n",
    "        return np_array, label\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.tensor(x)),  # Convert NumPy array to PyTorch tensor\n",
    "    transforms.Lambda(lambda x: x.unsqueeze(0)),   # Add channel dimension for CNN\n",
    "])\n",
    "\n",
    "# Path to your dataset\n",
    "dataset_path = r'C:\\Users\\Admin\\PycharmProjects\\Ramin_Thesis\\DataSet\\320by320\\12_class'\n",
    "\n",
    "# Create the dataset\n",
    "dataset = NumpyFolderDataset(root_dir=dataset_path, transform=transform)\n",
    "\n",
    "# Split into training and test datasets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Verify DataLoader\n",
    "for batch_data, batch_labels in train_loader:\n",
    "    print(f\"Batch data shape: {batch_data.shape}, Batch labels: {batch_labels.shape}\")\n",
    "    break"
   ],
   "id": "6bb9687145a8c3d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch data shape: torch.Size([32, 1, 320, 320]), Batch labels: torch.Size([32])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1",
   "id": "a495e295e2a7bc0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T19:14:00.906744Z",
     "start_time": "2024-12-14T19:14:00.894679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ImprovedCNNWithTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=8, num_transformer_layers=4, num_heads=16):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=2),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Transformer Parameters\n",
    "        self.embed_dim = 128  # Token embedding size\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((4, 4))  # Fixed-size output for tokenization\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=self.embed_dim, nhead=num_heads, dim_feedforward=512, dropout=0.4)\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=num_transformer_layers)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(self.embed_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = F.gelu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Block 2\n",
    "        x = F.gelu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Block 3 with Residual\n",
    "        shortcut = self.shortcut(x)  # Downsample shortcut\n",
    "        x = F.gelu(self.bn3(self.conv3(x)) + shortcut)\n",
    "\n",
    "        # Global Pooling\n",
    "        x = self.global_pool(x)  # Shape: [batch_size, 128, 4, 4]\n",
    "        batch_size, channels, height, width = x.size()\n",
    "\n",
    "        # Prepare Transformer Input\n",
    "        x = x.view(batch_size, channels, -1).permute(0, 2, 1)  # Shape: [batch, 16, 128]\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer(x)  # Shape: [batch, 16, 128]\n",
    "        x = x.mean(dim=1)  # Aggregate token representations (Shape: [batch, 128])\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = self.dropout(F.gelu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "model = ImprovedCNNWithTransformer()"
   ],
   "id": "a7e101c094de8e41",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Eval Section:",
   "id": "900a62767234fcaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T22:12:27.543853Z",
     "start_time": "2024-12-14T19:14:04.456561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "num_epochs = 100  # Assuming a total of 50 epochs\n",
    "model.to(device)\n",
    "\n",
    "# Variable to track the best model\n",
    "best_test_accuracy = 0.0\n",
    "best_model_path = \"best_model.pth\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Training loop\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate training metrics\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, \"\n",
    "          f\"Accuracy: {epoch_accuracy:.2%}, \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # Start testing after 5th epoch\n",
    "    if epoch >= 3:\n",
    "        model.eval()\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "        # Calculate test accuracy\n",
    "        test_accuracy = correct_predictions / total_samples\n",
    "        print(f\"Test Accuracy after Epoch {epoch+1}: {test_accuracy:.2%}\")\n",
    "\n",
    "        # Save the model if it's the best so far\n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"###################### Best model saved with accuracy ######################: {best_test_accuracy:.2%}\")\n",
    "\n",
    "# Load the best model for further use\n",
    "print(f\"Training complete. Best model accuracy: {best_test_accuracy:.2%}\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n"
   ],
   "id": "4226cfcc71f7c9c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.4947, Accuracy: 36.93%, LR: 0.000300\n",
      "Epoch [2/100], Loss: 0.9868, Accuracy: 59.95%, LR: 0.000300\n",
      "Epoch [3/100], Loss: 0.8177, Accuracy: 66.99%, LR: 0.000300\n",
      "Epoch [4/100], Loss: 0.7589, Accuracy: 69.37%, LR: 0.000300\n",
      "Test Accuracy after Epoch 4: 74.84%\n",
      "###################### Best model saved with accuracy ######################: 74.84%\n",
      "Epoch [5/100], Loss: 0.6855, Accuracy: 71.99%, LR: 0.000300\n",
      "Test Accuracy after Epoch 5: 64.10%\n",
      "Epoch [6/100], Loss: 0.6697, Accuracy: 73.17%, LR: 0.000300\n",
      "Test Accuracy after Epoch 6: 77.08%\n",
      "###################### Best model saved with accuracy ######################: 77.08%\n",
      "Epoch [7/100], Loss: 0.5384, Accuracy: 78.65%, LR: 0.000300\n",
      "Test Accuracy after Epoch 7: 78.21%\n",
      "###################### Best model saved with accuracy ######################: 78.21%\n",
      "Epoch [8/100], Loss: 0.4920, Accuracy: 80.63%, LR: 0.000300\n",
      "Test Accuracy after Epoch 8: 81.25%\n",
      "###################### Best model saved with accuracy ######################: 81.25%\n",
      "Epoch [9/100], Loss: 0.4017, Accuracy: 84.54%, LR: 0.000300\n",
      "Test Accuracy after Epoch 9: 84.46%\n",
      "###################### Best model saved with accuracy ######################: 84.46%\n",
      "Epoch [10/100], Loss: 0.4072, Accuracy: 84.21%, LR: 0.000300\n",
      "Test Accuracy after Epoch 10: 87.82%\n",
      "###################### Best model saved with accuracy ######################: 87.82%\n",
      "Epoch [11/100], Loss: 0.4141, Accuracy: 84.35%, LR: 0.000300\n",
      "Test Accuracy after Epoch 11: 83.01%\n",
      "Epoch [12/100], Loss: 0.3356, Accuracy: 87.23%, LR: 0.000300\n",
      "Test Accuracy after Epoch 12: 90.54%\n",
      "###################### Best model saved with accuracy ######################: 90.54%\n",
      "Epoch [13/100], Loss: 0.3264, Accuracy: 87.87%, LR: 0.000300\n",
      "Test Accuracy after Epoch 13: 90.71%\n",
      "###################### Best model saved with accuracy ######################: 90.71%\n",
      "Epoch [14/100], Loss: 0.3235, Accuracy: 87.68%, LR: 0.000300\n",
      "Test Accuracy after Epoch 14: 86.22%\n",
      "Epoch [15/100], Loss: 0.2952, Accuracy: 88.82%, LR: 0.000300\n",
      "Test Accuracy after Epoch 15: 87.18%\n",
      "Epoch [16/100], Loss: 0.2907, Accuracy: 88.28%, LR: 0.000300\n",
      "Test Accuracy after Epoch 16: 91.83%\n",
      "###################### Best model saved with accuracy ######################: 91.83%\n",
      "Epoch [17/100], Loss: 0.2657, Accuracy: 89.32%, LR: 0.000300\n",
      "Test Accuracy after Epoch 17: 91.35%\n",
      "Epoch [18/100], Loss: 0.2654, Accuracy: 89.60%, LR: 0.000300\n",
      "Test Accuracy after Epoch 18: 90.87%\n",
      "Epoch [19/100], Loss: 0.2467, Accuracy: 90.56%, LR: 0.000300\n",
      "Test Accuracy after Epoch 19: 79.81%\n",
      "Epoch [20/100], Loss: 0.2415, Accuracy: 90.72%, LR: 0.000300\n",
      "Test Accuracy after Epoch 20: 91.35%\n",
      "Epoch [21/100], Loss: 0.2530, Accuracy: 90.06%, LR: 0.000300\n",
      "Test Accuracy after Epoch 21: 87.98%\n",
      "Epoch [22/100], Loss: 0.2251, Accuracy: 91.33%, LR: 0.000300\n",
      "Test Accuracy after Epoch 22: 90.38%\n",
      "Epoch [23/100], Loss: 0.2286, Accuracy: 91.27%, LR: 0.000300\n",
      "Test Accuracy after Epoch 23: 89.90%\n",
      "Epoch [24/100], Loss: 0.2367, Accuracy: 90.92%, LR: 0.000300\n",
      "Test Accuracy after Epoch 24: 92.31%\n",
      "###################### Best model saved with accuracy ######################: 92.31%\n",
      "Epoch [25/100], Loss: 0.2088, Accuracy: 91.97%, LR: 0.000300\n",
      "Test Accuracy after Epoch 25: 88.94%\n",
      "Epoch [26/100], Loss: 0.2318, Accuracy: 91.38%, LR: 0.000300\n",
      "Test Accuracy after Epoch 26: 89.10%\n",
      "Epoch [27/100], Loss: 0.2021, Accuracy: 92.49%, LR: 0.000300\n",
      "Test Accuracy after Epoch 27: 91.19%\n",
      "Epoch [28/100], Loss: 0.2018, Accuracy: 92.22%, LR: 0.000300\n",
      "Test Accuracy after Epoch 28: 90.06%\n",
      "Epoch [29/100], Loss: 0.2113, Accuracy: 91.86%, LR: 0.000300\n",
      "Test Accuracy after Epoch 29: 88.14%\n",
      "Epoch [30/100], Loss: 0.2069, Accuracy: 91.90%, LR: 0.000300\n",
      "Test Accuracy after Epoch 30: 90.38%\n",
      "Epoch [31/100], Loss: 0.2056, Accuracy: 91.52%, LR: 0.000300\n",
      "Test Accuracy after Epoch 31: 89.90%\n",
      "Epoch [32/100], Loss: 0.2009, Accuracy: 91.86%, LR: 0.000300\n",
      "Test Accuracy after Epoch 32: 92.63%\n",
      "###################### Best model saved with accuracy ######################: 92.63%\n",
      "Epoch [33/100], Loss: 0.1925, Accuracy: 92.27%, LR: 0.000300\n",
      "Test Accuracy after Epoch 33: 92.31%\n",
      "Epoch [34/100], Loss: 0.1861, Accuracy: 93.20%, LR: 0.000300\n",
      "Test Accuracy after Epoch 34: 91.03%\n",
      "Epoch [35/100], Loss: 0.1991, Accuracy: 92.18%, LR: 0.000300\n",
      "Test Accuracy after Epoch 35: 92.95%\n",
      "###################### Best model saved with accuracy ######################: 92.95%\n",
      "Epoch [36/100], Loss: 0.2066, Accuracy: 92.34%, LR: 0.000300\n",
      "Test Accuracy after Epoch 36: 91.83%\n",
      "Epoch [37/100], Loss: 0.1679, Accuracy: 93.61%, LR: 0.000300\n",
      "Test Accuracy after Epoch 37: 91.99%\n",
      "Epoch [38/100], Loss: 0.1771, Accuracy: 93.43%, LR: 0.000300\n",
      "Test Accuracy after Epoch 38: 92.31%\n",
      "Epoch [39/100], Loss: 0.1498, Accuracy: 94.57%, LR: 0.000300\n",
      "Test Accuracy after Epoch 39: 89.42%\n",
      "Epoch [40/100], Loss: 0.1688, Accuracy: 93.57%, LR: 0.000300\n",
      "Test Accuracy after Epoch 40: 91.51%\n",
      "Epoch [41/100], Loss: 0.1762, Accuracy: 93.43%, LR: 0.000300\n",
      "Test Accuracy after Epoch 41: 87.82%\n",
      "Epoch [42/100], Loss: 0.1846, Accuracy: 93.39%, LR: 0.000300\n",
      "Test Accuracy after Epoch 42: 88.62%\n",
      "Epoch [43/100], Loss: 0.1865, Accuracy: 92.88%, LR: 0.000300\n",
      "Test Accuracy after Epoch 43: 92.63%\n",
      "Epoch [44/100], Loss: 0.1476, Accuracy: 94.37%, LR: 0.000300\n",
      "Test Accuracy after Epoch 44: 91.03%\n",
      "Epoch [45/100], Loss: 0.1483, Accuracy: 94.39%, LR: 0.000300\n",
      "Test Accuracy after Epoch 45: 90.87%\n",
      "Epoch [46/100], Loss: 0.1316, Accuracy: 94.91%, LR: 0.000300\n",
      "Test Accuracy after Epoch 46: 92.31%\n",
      "Epoch [47/100], Loss: 0.1494, Accuracy: 94.00%, LR: 0.000300\n",
      "Test Accuracy after Epoch 47: 92.31%\n",
      "Epoch [48/100], Loss: 0.1348, Accuracy: 94.87%, LR: 0.000300\n",
      "Test Accuracy after Epoch 48: 92.31%\n",
      "Epoch [49/100], Loss: 0.1414, Accuracy: 94.87%, LR: 0.000300\n",
      "Test Accuracy after Epoch 49: 89.10%\n",
      "Epoch [50/100], Loss: 0.1317, Accuracy: 94.80%, LR: 0.000300\n",
      "Test Accuracy after Epoch 50: 91.51%\n",
      "Epoch [51/100], Loss: 0.1333, Accuracy: 94.66%, LR: 0.000300\n",
      "Test Accuracy after Epoch 51: 92.15%\n",
      "Epoch [52/100], Loss: 0.1261, Accuracy: 95.25%, LR: 0.000300\n",
      "Test Accuracy after Epoch 52: 89.74%\n",
      "Epoch [53/100], Loss: 0.1307, Accuracy: 95.03%, LR: 0.000300\n",
      "Test Accuracy after Epoch 53: 92.15%\n",
      "Epoch [54/100], Loss: 0.1211, Accuracy: 95.53%, LR: 0.000300\n",
      "Test Accuracy after Epoch 54: 92.95%\n",
      "Epoch [55/100], Loss: 0.1268, Accuracy: 95.37%, LR: 0.000300\n",
      "Test Accuracy after Epoch 55: 91.19%\n",
      "Epoch [56/100], Loss: 0.1295, Accuracy: 95.17%, LR: 0.000300\n",
      "Test Accuracy after Epoch 56: 91.19%\n",
      "Epoch [57/100], Loss: 0.1229, Accuracy: 95.03%, LR: 0.000300\n",
      "Test Accuracy after Epoch 57: 91.19%\n",
      "Epoch [58/100], Loss: 0.1172, Accuracy: 95.71%, LR: 0.000300\n",
      "Test Accuracy after Epoch 58: 92.95%\n",
      "Epoch [59/100], Loss: 0.1069, Accuracy: 96.10%, LR: 0.000300\n",
      "Test Accuracy after Epoch 59: 91.03%\n",
      "Epoch [60/100], Loss: 0.1150, Accuracy: 95.71%, LR: 0.000300\n",
      "Test Accuracy after Epoch 60: 91.51%\n",
      "Epoch [61/100], Loss: 0.1215, Accuracy: 95.58%, LR: 0.000300\n",
      "Test Accuracy after Epoch 61: 92.15%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 23\u001B[0m\n\u001B[0;32m     20\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     22\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 23\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[0;32m     24\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m     25\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[4], line 34\u001B[0m, in \u001B[0;36mImprovedCNNWithTransformer.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;66;03m# Block 1\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mgelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn1(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv1(x)))\n\u001B[0;32m     35\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmax_pool2d(x, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;66;03m# Block 2\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    553\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 554\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conv_forward(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\n\u001B[0;32m    539\u001B[0m         F\u001B[38;5;241m.\u001B[39mpad(\n\u001B[0;32m    540\u001B[0m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    547\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups,\n\u001B[0;32m    548\u001B[0m     )\n\u001B[1;32m--> 549\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\n\u001B[0;32m    550\u001B[0m     \u001B[38;5;28minput\u001B[39m, weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups\n\u001B[0;32m    551\u001B[0m )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T22:15:21.013580Z",
     "start_time": "2024-12-14T22:15:14.689046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# Define your class names manually\n",
    "class_names = ([\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\",\n",
    "               \"Class_5\", \"Class_6\", \"Class_7\", \"Class_8\"])\n",
    "\n",
    "# \"Class_9\", \"Class_10\", \"Class_11\", \"Class_12\"]  # Replace with actual class names\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Convert to numpy arrays for evaluation\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Overall accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Overall Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Per-class accuracy\n",
    "    class_accuracy = np.diag(confusion_matrix(all_labels, all_preds)) / np.bincount(all_labels)\n",
    "    for i, acc in enumerate(class_accuracy):\n",
    "        print(f\"Class {i} Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Test the model\n",
    "test_accuracy = evaluate_model(model, test_loader, device)\n"
   ],
   "id": "756b680396be2ae3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 20/20 [00:06<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Test Accuracy: 0.9215\n",
      "Class 0 Accuracy: 0.9706\n",
      "Class 1 Accuracy: 0.8427\n",
      "Class 2 Accuracy: 0.9565\n",
      "Class 3 Accuracy: 0.9865\n",
      "Class 4 Accuracy: 1.0000\n",
      "Class 5 Accuracy: 0.9733\n",
      "Class 6 Accuracy: 0.7976\n",
      "Class 7 Accuracy: 0.8750\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class_1       0.96      0.97      0.96        68\n",
      "     Class_2       0.87      0.84      0.86        89\n",
      "     Class_3       0.98      0.96      0.97        92\n",
      "     Class_4       0.90      0.99      0.94        74\n",
      "     Class_5       0.99      1.00      0.99        70\n",
      "     Class_6       0.90      0.97      0.94        75\n",
      "     Class_7       0.99      0.80      0.88        84\n",
      "     Class_8       0.81      0.88      0.84        72\n",
      "\n",
      "    accuracy                           0.92       624\n",
      "   macro avg       0.92      0.93      0.92       624\n",
      "weighted avg       0.92      0.92      0.92       624\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[66  0  2  0  0  0  0  0]\n",
      " [ 0 75  0  0  0  0  1 13]\n",
      " [ 3  0 88  0  1  0  0  0]\n",
      " [ 0  0  0 73  0  1  0  0]\n",
      " [ 0  0  0  0 70  0  0  0]\n",
      " [ 0  0  0  2  0 73  0  0]\n",
      " [ 0  3  0  6  0  6 67  2]\n",
      " [ 0  8  0  0  0  1  0 63]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
