{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model 2",
   "id": "97b7528a4f4a4811"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:49:29.777365Z",
     "start_time": "2024-12-09T12:49:29.774007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ],
   "id": "2e67c708c52cbd9e",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## initiation:",
   "id": "2784339f6be1f305"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:03:23.197540Z",
     "start_time": "2024-12-09T12:03:23.025309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NumpyFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset to handle .npy files organized by class in subdirectories.\n",
    "\n",
    "        Args:\n",
    "        - root_dir (str): Root directory containing class subdirectories.\n",
    "        - transform (callable, optional): Transform to apply to each sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []  # List to store file paths and their labels\n",
    "\n",
    "        # Traverse through the directory structure\n",
    "        for class_idx, class_name in enumerate(sorted(os.listdir(root_dir))):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for file_name in os.listdir(class_dir):\n",
    "                    if file_name.endswith('.npy'):\n",
    "                        file_path = os.path.join(class_dir, file_name)\n",
    "                        self.samples.append((file_path, class_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.samples[idx]\n",
    "        np_array = np.load(file_path).astype(np.float32)  # Load the .npy file\n",
    "\n",
    "        if self.transform:\n",
    "            np_array = self.transform(np_array)\n",
    "        return np_array, label\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.tensor(x)),  # Convert NumPy array to PyTorch tensor\n",
    "    transforms.Lambda(lambda x: x.unsqueeze(0)),   # Add channel dimension for CNN\n",
    "])\n",
    "\n",
    "# Path to your dataset\n",
    "dataset_path = r'G:\\Thesis_Numpy_data_set\\2_Class_160'\n",
    "\n",
    "# Create the dataset\n",
    "dataset = NumpyFolderDataset(root_dir=dataset_path, transform=transform)\n",
    "\n",
    "# Split into training and test datasets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Verify DataLoader\n",
    "for batch_data, batch_labels in train_loader:\n",
    "    print(f\"Batch data shape: {batch_data.shape}, Batch labels: {batch_labels.shape}\")\n",
    "    break"
   ],
   "id": "6bb9687145a8c3d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch data shape: torch.Size([32, 1, 160, 160]), Batch labels: torch.Size([32])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1",
   "id": "a495e295e2a7bc0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:31:23.354161Z",
     "start_time": "2024-12-09T12:31:23.342301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ImprovedCNNWithTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=2, num_transformer_layers=4, num_heads=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=2),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Transformer Parameters\n",
    "        self.embed_dim = 128  # Token embedding size\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((4, 4))  # Fixed-size output for tokenization\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=self.embed_dim, nhead=num_heads, dim_feedforward=512, dropout=0.4)\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=num_transformer_layers)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(self.embed_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Block 2\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Block 3 with Residual\n",
    "        shortcut = self.shortcut(x)  # Downsample shortcut\n",
    "        x = F.relu(self.bn3(self.conv3(x)) + shortcut)\n",
    "\n",
    "        # Global Pooling\n",
    "        x = self.global_pool(x)  # Shape: [batch_size, 128, 4, 4]\n",
    "        batch_size, channels, height, width = x.size()\n",
    "\n",
    "        # Prepare Transformer Input\n",
    "        x = x.view(batch_size, channels, -1).permute(0, 2, 1)  # Shape: [batch, 16, 128]\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer(x)  # Shape: [batch, 16, 128]\n",
    "        x = x.mean(dim=1)  # Aggregate token representations (Shape: [batch, 128])\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "model = ImprovedCNNWithTransformer()"
   ],
   "id": "a7e101c094de8e41",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Eval Section:",
   "id": "900a62767234fcaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:41:14.481963Z",
     "start_time": "2024-12-09T12:33:10.967215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Updated optimizer with weight decay\n",
    "# optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "num_epochs = 50  # Adjust as needed\n",
    "model.to(device)  # Ensure the model is moved to the device\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Training loop\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "\n",
    "        # Forward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Print detailed epoch metrics\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, \"\n",
    "          f\"Accuracy: {epoch_accuracy:.2%}, \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "# Print test accuracy\n",
    "test_accuracy = correct_predictions / total_samples\n",
    "print(f\"Test Accuracy: {test_accuracy:.2%}\")\n"
   ],
   "id": "3ead4de42c260910",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.4567, Accuracy: 79.13%, LR: 0.000300\n",
      "Epoch [2/50], Loss: 0.3986, Accuracy: 82.41%, LR: 0.000300\n",
      "Epoch [3/50], Loss: 0.3769, Accuracy: 83.69%, LR: 0.000300\n",
      "Epoch [4/50], Loss: 0.3217, Accuracy: 86.61%, LR: 0.000300\n",
      "Epoch [5/50], Loss: 0.3055, Accuracy: 87.04%, LR: 0.000300\n",
      "Epoch [6/50], Loss: 0.3002, Accuracy: 88.03%, LR: 0.000300\n",
      "Epoch [7/50], Loss: 0.2689, Accuracy: 88.75%, LR: 0.000300\n",
      "Epoch [8/50], Loss: 0.2839, Accuracy: 88.25%, LR: 0.000300\n",
      "Epoch [9/50], Loss: 0.2684, Accuracy: 88.53%, LR: 0.000300\n",
      "Epoch [10/50], Loss: 0.2267, Accuracy: 90.03%, LR: 0.000300\n",
      "Epoch [11/50], Loss: 0.2048, Accuracy: 91.67%, LR: 0.000300\n",
      "Epoch [12/50], Loss: 0.1860, Accuracy: 92.45%, LR: 0.000300\n",
      "Epoch [13/50], Loss: 0.1926, Accuracy: 92.24%, LR: 0.000300\n",
      "Epoch [14/50], Loss: 0.2020, Accuracy: 91.45%, LR: 0.000300\n",
      "Epoch [15/50], Loss: 0.1734, Accuracy: 93.16%, LR: 0.000300\n",
      "Epoch [16/50], Loss: 0.1643, Accuracy: 94.16%, LR: 0.000300\n",
      "Epoch [17/50], Loss: 0.1572, Accuracy: 93.87%, LR: 0.000300\n",
      "Epoch [18/50], Loss: 0.1576, Accuracy: 93.45%, LR: 0.000300\n",
      "Epoch [19/50], Loss: 0.1460, Accuracy: 94.02%, LR: 0.000300\n",
      "Epoch [20/50], Loss: 0.1305, Accuracy: 95.01%, LR: 0.000300\n",
      "Epoch [21/50], Loss: 0.1808, Accuracy: 93.38%, LR: 0.000300\n",
      "Epoch [22/50], Loss: 0.1368, Accuracy: 94.23%, LR: 0.000300\n",
      "Epoch [23/50], Loss: 0.1443, Accuracy: 94.02%, LR: 0.000300\n",
      "Epoch [24/50], Loss: 0.1123, Accuracy: 95.80%, LR: 0.000300\n",
      "Epoch [25/50], Loss: 0.1138, Accuracy: 95.44%, LR: 0.000300\n",
      "Epoch [26/50], Loss: 0.1097, Accuracy: 96.01%, LR: 0.000300\n",
      "Epoch [27/50], Loss: 0.0887, Accuracy: 95.80%, LR: 0.000300\n",
      "Epoch [28/50], Loss: 0.1228, Accuracy: 95.16%, LR: 0.000300\n",
      "Epoch [29/50], Loss: 0.1018, Accuracy: 96.01%, LR: 0.000300\n",
      "Epoch [30/50], Loss: 0.0717, Accuracy: 97.29%, LR: 0.000300\n",
      "Epoch [31/50], Loss: 0.0855, Accuracy: 96.58%, LR: 0.000300\n",
      "Epoch [32/50], Loss: 0.1080, Accuracy: 96.15%, LR: 0.000300\n",
      "Epoch [33/50], Loss: 0.0673, Accuracy: 97.29%, LR: 0.000300\n",
      "Epoch [34/50], Loss: 0.0777, Accuracy: 97.29%, LR: 0.000300\n",
      "Epoch [35/50], Loss: 0.0816, Accuracy: 97.22%, LR: 0.000300\n",
      "Epoch [36/50], Loss: 0.1114, Accuracy: 95.73%, LR: 0.000300\n",
      "Epoch [37/50], Loss: 0.0682, Accuracy: 97.29%, LR: 0.000300\n",
      "Epoch [38/50], Loss: 0.0606, Accuracy: 97.79%, LR: 0.000300\n",
      "Epoch [39/50], Loss: 0.0659, Accuracy: 98.01%, LR: 0.000300\n",
      "Epoch [40/50], Loss: 0.0625, Accuracy: 97.01%, LR: 0.000300\n",
      "Epoch [41/50], Loss: 0.0872, Accuracy: 96.79%, LR: 0.000300\n",
      "Epoch [42/50], Loss: 0.0479, Accuracy: 98.15%, LR: 0.000300\n",
      "Epoch [43/50], Loss: 0.0400, Accuracy: 98.36%, LR: 0.000300\n",
      "Epoch [44/50], Loss: 0.0604, Accuracy: 97.29%, LR: 0.000300\n",
      "Epoch [45/50], Loss: 0.0391, Accuracy: 98.72%, LR: 0.000300\n",
      "Epoch [46/50], Loss: 0.0535, Accuracy: 98.01%, LR: 0.000300\n",
      "Epoch [47/50], Loss: 0.0652, Accuracy: 97.08%, LR: 0.000300\n",
      "Epoch [48/50], Loss: 0.0291, Accuracy: 99.07%, LR: 0.000300\n",
      "Epoch [49/50], Loss: 0.0550, Accuracy: 98.01%, LR: 0.000300\n",
      "Epoch [50/50], Loss: 0.0627, Accuracy: 97.93%, LR: 0.000300\n",
      "Test Accuracy: 92.95%\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:49:36.362125Z",
     "start_time": "2024-12-09T12:49:35.870328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Convert to numpy arrays for evaluation\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Overall accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Overall Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Per-class accuracy\n",
    "    class_accuracy = np.diag(confusion_matrix(all_labels, all_preds)) / np.bincount(all_labels)\n",
    "    for i, acc in enumerate(class_accuracy):\n",
    "        print(f\"Class {i} Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=dataset.classes))\n",
    "\n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Test the model\n",
    "test_accuracy = evaluate_model(model, test_loader, device)"
   ],
   "id": "c35e03c5e1485637",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 11.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Test Accuracy: 0.9295\n",
      "Class 0 Accuracy: 0.8974\n",
      "Class 1 Accuracy: 0.9615\n",
      "\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NumpyFolderDataset' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 43\u001B[0m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m accuracy\n\u001B[0;32m     42\u001B[0m \u001B[38;5;66;03m# Test the model\u001B[39;00m\n\u001B[1;32m---> 43\u001B[0m test_accuracy \u001B[38;5;241m=\u001B[39m evaluate_model(model, test_loader, device)\n",
      "Cell \u001B[1;32mIn[25], line 34\u001B[0m, in \u001B[0;36mevaluate_model\u001B[1;34m(model, test_loader, device)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# Classification report\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mClassification Report:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 34\u001B[0m \u001B[38;5;28mprint\u001B[39m(classification_report(all_labels, all_preds, target_names\u001B[38;5;241m=\u001B[39mdataset\u001B[38;5;241m.\u001B[39mclasses))\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# Confusion matrix\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mConfusion Matrix:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NumpyFolderDataset' object has no attribute 'classes'"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "756b680396be2ae3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
