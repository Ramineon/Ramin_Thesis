{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model 3",
   "id": "6786d1f26ef92f14"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-14T08:58:37.245057Z",
     "start_time": "2024-12-14T08:58:35.081617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import DataLoader, Subset, random_split, Dataset"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## initiation:",
   "id": "879cccef9c2617e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:58:37.272160Z",
     "start_time": "2024-12-14T08:58:37.247063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NumpyFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for class_idx, class_name in enumerate(sorted(os.listdir(root_dir))):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                if file_name.endswith('.npy'):\n",
    "                    file_path = os.path.join(class_dir, file_name)\n",
    "                    self.samples.append((file_path, class_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.samples[idx]\n",
    "        np_array = np.load(file_path).astype(np.float32)\n",
    "        if self.transform:\n",
    "            np_array = self.transform(np_array)\n",
    "        return np_array, label\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.tensor(x)),\n",
    "    transforms.Lambda(lambda x: x.unsqueeze(0)),  # Add channel dimension\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,)),  # Normalize\n",
    "])\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = r'C:\\Users\\Admin\\PycharmProjects\\Ramin_Thesis\\DataSet\\12_class'\n",
    "dataset = NumpyFolderDataset(root_dir=dataset_path, transform=transform)\n",
    "\n",
    "# Train-test split\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Verify DataLoader\n",
    "for batch_data, batch_labels in train_loader:\n",
    "    print(f\"Batch data shape: {batch_data.shape}, Batch labels: {batch_labels.shape}\")\n",
    "    break\n"
   ],
   "id": "b97d4bf0d509c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch data shape: torch.Size([32, 1, 320, 320]), Batch labels: torch.Size([32])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1_from 3   ozmnenei",
   "id": "c7c6bc21c7c77397"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T08:58:37.847284Z",
     "start_time": "2024-12-14T08:58:37.317250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RecurrencePlotClassifierScratch(nn.Module):\n",
    "    def __init__(self, num_classes, transformer_layers=2, transformer_heads=4, transformer_dim=128, dropout=0.1):\n",
    "        super(RecurrencePlotClassifierScratch, self).__init__()\n",
    "\n",
    "        # CNN Feature Extractor (from scratch)\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # Input channels: 3\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1) # Global Average Pooling\n",
    "        )\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.transformer_input_dim = 256\n",
    "        self.transformer_embedding = nn.Linear(self.transformer_input_dim, transformer_dim)\n",
    "        self.positional_encoding = PositionalEncoding(transformer_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=transformer_dim, nhead=transformer_heads, dropout=dropout),\n",
    "            num_layers=transformer_layers\n",
    "        )\n",
    "\n",
    "        # Classification Head\n",
    "        self.fc = nn.Linear(transformer_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # CNN Feature Extraction\n",
    "        features = self.cnn_layers(x)\n",
    "        features = features.view(batch_size, -1)\n",
    "\n",
    "        # Prepare for Transformer\n",
    "        features = features.unsqueeze(1)  # Add sequence dimension\n",
    "        features = self.transformer_embedding(features)\n",
    "        features = self.positional_encoding(features)\n",
    "\n",
    "        # Transformer\n",
    "        transformer_output = self.transformer_encoder(features)\n",
    "\n",
    "        # Classification\n",
    "        transformer_output = transformer_output.squeeze(1)\n",
    "        output = self.dropout(transformer_output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# Positional Encoding (same as before)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Example usage:\n",
    "num_classes = 2\n",
    "model = RecurrencePlotClassifierScratch(num_classes)\n",
    "input_tensor = torch.randn(32, 1, 320, 320)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)"
   ],
   "id": "9b6bc6bf545b1d6e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-14T08:58:37.856849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (Your model definition and data loading code from previous responses) ...\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True) # Learning rate scheduler\n",
    "num_epochs = 100\n",
    "patience = 10  # Patience for early stopping\n",
    "best_test_accuracy = 0.0\n",
    "best_model_path = \"best_model.pth\"\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, \"\n",
    "          f\"Accuracy: {epoch_accuracy:.2%}, \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    if epoch >= 3:  # Start testing after a few epochs\n",
    "        model.eval()\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "        test_accuracy = correct_predictions / total_samples\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        print(f\"Test Accuracy after Epoch {epoch+1}: {test_accuracy:.2%}\")\n",
    "\n",
    "        scheduler.step(test_accuracy)  # Update learning rate based on test accuracy\n",
    "\n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"###################### Best model saved with accuracy ######################: {best_test_accuracy:.2%}\")\n",
    "            early_stopping_counter = 0  # Reset early stopping counter\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "                break  # Exit training loop\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Total training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and test accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training complete. Best test accuracy: {best_test_accuracy:.2%}\")\n",
    "model.load_state_dict(torch.load(best_model_path))"
   ],
   "id": "4bc22b26ee6d8dd3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ce4b2d52a751837d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
